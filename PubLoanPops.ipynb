{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PubLoanPops.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xdderekchen/public_loan_data/blob/master/PubLoanPops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaGCL2ls5B_6",
        "colab_type": "text"
      },
      "source": [
        "<h1>ETL for public loan data from FNMA and FRED</h1>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0xSKTFwTSC3",
        "colab_type": "text"
      },
      "source": [
        "#  1. Introduction\n",
        "\n",
        "A python implementation for processing public loan performance data from FNMA and FRED is presented here.\n",
        "\n",
        "Features:\n",
        "  - handling data from FNMA and FRED\n",
        "  - implemented in both pandas and pyspark\n",
        "  - results can be saved to parquet or sqlite.\n",
        "\n",
        "Data Sources:\n",
        "- FNMA Loan Data can be accessed from https://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html\n",
        "- FRED Loan Data can be accessed from http://www.freddiemac.com/research/datasets/sf_loanlevel_dataset.page\n",
        "\n",
        "Ideally the source code should be packed into a python package, however in this early stage of implementation, I find it is easier to put all codes in the multiple sessions all in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G8INjheOAPu",
        "colab_type": "text"
      },
      "source": [
        "# 2. Core Source Code\n",
        "In this section, the core code (classes) and testing code are present.\n",
        "* 2.1 utility functions\n",
        "* 2.2 class Data_Schema\n",
        "* 2.3 class Agency_Loan\n",
        " * 2.3.1 implementation\n",
        " * 2.3.1 unit test\n",
        "* 2.4 class Agency_Loan_Dask\n",
        " * 2.4.1 implementation\n",
        " * 2.4.2 unit test\n",
        "* 2.5 class Agency_Loan_Sark\n",
        " * 2.5.1 implementation\n",
        " * 2.5.1 unit test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnNzomCQHBfM",
        "colab_type": "text"
      },
      "source": [
        "  ##  2.1 Utility Functions\n",
        "  Several utitity functions, like to measure the runtime duration, calculate amortization of loans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSHKkmm2twAD",
        "colab_type": "code",
        "outputId": "46b00577-168c-4a01-cc09-20a63373d7c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "# Code to mount my Google Drive to colab. \n",
        "# The input data and output data are stored in Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4LP1KmjxFqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get codes from github\n",
        "! git clone https://github.com/xdderekchen/public_loan_data.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3IlHVUPHMi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def showtime(tstart):\n",
        "    \"\"\"\n",
        "    Show runtime duration since tstart\n",
        "\n",
        "    parameters\n",
        "    ----------\n",
        "    tstart: datetime, since this time, the duration is calculated\n",
        "    Returns\n",
        "    -------\n",
        "    out : duration in ms\n",
        "    \"\"\"\n",
        "    te = time.time()\n",
        "    return f\"{int((te - tstart) * 1000)} ms\"\n",
        "\n",
        "def decorator_time(method):\n",
        "    \"\"\"\n",
        "    Decorator function. Show runtime duration for the wrapped function.\n",
        "    \"\"\"\n",
        "    def timed(*args, **kw):\n",
        "        ts = time.time()\n",
        "        result = method(*args, **kw)\n",
        "        te = time.time()\n",
        "        if 'log_time' in kw:\n",
        "            name = kw.get('log_name', method.__name__.upper())\n",
        "            kw['log_time'][name] = f\"{int((te - ts) * 1000)} ms\"\n",
        "        else:\n",
        "            print('%r  %2.2f ms' % \\\n",
        "                  (method.__name__, (te - ts) * 1000)\n",
        "                  )\n",
        "        return result\n",
        "    return timed\n",
        "\n",
        "\n",
        "def compute_amortization(principals, monthly_rates, terms,  start_period = 0, end_period = None):\n",
        "    \"\"\"\n",
        "    Compute amortization of loans\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    principals : scalar or array_like of shape(M, )\n",
        "        pricipals of loans\n",
        "    monthly_rates: scalar or array_like if  principals is a scalar\n",
        "                   or\n",
        "                   array_like or matrix_like if principals is an array_like\n",
        "        For FRM, one Rate for each loan\n",
        "        For ARM, one full time series for each loan\n",
        "    \n",
        "    terms:  scalar or array_like of shape(M, )\n",
        "    loan terms, type should match that of principals\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    out : ndarray (M, N)\n",
        "\n",
        "    \"\"\"\n",
        "    num_loans = 1\n",
        "    if isinstance(principals, pd.Series):\n",
        "       principals = principals.values\n",
        "    elif np.ndim(principals) == 0:\n",
        "       principals = [principals]\n",
        "       \n",
        "    #assume principals is np.array\n",
        "    num_loans = len(principals)\n",
        "\n",
        "    if isinstance(monthly_rates, pd.Series):\n",
        "       monthly_rates = monthly_rates.values\n",
        "    elif np.ndim(principals) == 0:\n",
        "       monthly_rates = [monthly_rates]\n",
        "\n",
        "    if isinstance(terms, pd.Series):\n",
        "       terms = terms.values\n",
        "    elif np.ndim(terms) == 0:\n",
        "       terms = [terms]\n",
        "    \n",
        "    num_month = terms.max()\n",
        "    mini_term = terms.min()\n",
        "    if end_period is not None:\n",
        "      num_month = min(num_month, max(1, (end_period - start_period)))\n",
        "    \n",
        "    upb_matrix = np.zeros((num_month, num_loans))\n",
        "    the_payment = principals * (monthly_rates / (1 - (1 + monthly_rates) ** (-terms)))\n",
        "    \n",
        "    current_upb = principals\n",
        "    if start_period > 0:\n",
        "       current_upb = principals\n",
        "       for t in range(1, start_period+1):\n",
        "          pp_payment = the_payment - current_upb * monthly_rates\n",
        "          current_upb = current_upb - pp_payment\n",
        "    \n",
        "    upb_matrix[0, :] = current_upb\n",
        "      \n",
        "    i = 1\n",
        "    for t in range(start_period+1, start_period +num_month ):\n",
        "       isbeyondTerm = np.greater(t , terms)\n",
        "       pp_payment = the_payment - upb_matrix[i-1, :] * monthly_rates\n",
        "       upb_matrix[i, :] = (upb_matrix[i-1, :] - pp_payment) \n",
        "       if t >= mini_term:\n",
        "          iswithinTerm = np.greater( terms, t).astype(int)\n",
        "          isbeyondTerm = np.greater(t , terms).astype(int)\n",
        "          upb_matrix[i, :] = upb_matrix[i, :] * np.array(iswithinTerm) + (-999) * isbeyondTerm\n",
        "       i = i +  1\n",
        "    return upb_matrix.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmThbPdkR0e9",
        "colab_type": "text"
      },
      "source": [
        "  ## 2.2. class Data_Schema\n",
        "  The datasets provided by 2 agencies are simple csv files without header. You need to read the user guide to understand the meaning of columns. Here we put needed schema information together in the class Data_schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyD3vWYnUBN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data_Schema(object):\n",
        "    _AcquisitionSchema_FNMA = {\"LOAN_ID\":         {\"dtype\": \"string\"},\n",
        "                          \"ORIG_CHN\":        {\"dtype\": \"string\"},\n",
        "                          \"SellerName\":      {\"dtype\": \"string\", \"drop\":True},\n",
        "                          \"ORIG_RT\":         {\"dtype\": \"float\"},\n",
        "                          \"ORIG_AMT\":        {\"dtype\": \"double\"},\n",
        "                          \"ORIG_TRM\":        {\"dtype\": \"int\" },\n",
        "                          \"ORIG_DTE\":        {\"dtype\": \"date\", \"format\":\"%m/%Y\", \"format2\":\"MM/yyyy\"},\n",
        "                          \"FRST_DTE\":        {\"dtype\": \"date\", \"format\":\"%m/%Y\", \"format2\":\"MM/yyyy\"},\n",
        "                          \"OLTV\":            {\"dtype\": \"float\", \"default\": 0},\n",
        "                          \"OCLTV\":           {\"dtype\": \"float\", \"default\": 0},\n",
        "                          \"NUM_BO\":          {\"dtype\": \"int\", \"default\": -1},\n",
        "                          \"DTI\":             {\"dtype\": \"int\", \"default\": -1},\n",
        "                          \"CSCORE_B\":        {\"dtype\": \"float\", \"default\": -1},\n",
        "                          \"FTHB_FLG\":        {\"dtype\": \"string\"},\n",
        "                          \"PURPOSE\":         {\"dtype\": \"string\"},\n",
        "                          \"PROP_TYP\":        {\"dtype\": \"string\"},\n",
        "                          \"NUM_UNIT\":        {\"dtype\": \"int\", \"default\": -1},\n",
        "                          \"OCC_STAT\":        {\"dtype\": \"string\"},\n",
        "                          \"STATE\":           {\"dtype\": \"string\"},\n",
        "                          \"ZIP_3\":           {\"dtype\": \"string\"},\n",
        "                          \"MI_PCT\":          {\"dtype\": \"int\", \"default\": 0},\n",
        "                          \"Product_Type\":    {\"dtype\": \"string\"},\n",
        "                          \"CSCORE_C\":        {\"dtype\": \"int\", \"default\": -1},\n",
        "                          \"MI_TYPE\":         {\"dtype\": \"string\", \"default\": \"0\"},\n",
        "                          \"RELOCATION_FLG\":  {\"dtype\": \"string\"}\n",
        "                          }\n",
        "\n",
        "    # schema of Performance Data\n",
        "    _PerformanceSchema_FNMA = {\"LOAN_ID\":         {\"dtype\": \"string\"},\n",
        "                          \"ACT_DTE\":         {\"dtype\": \"date\", \"format\":\"%d/%m/%Y\", \"format2\":\"MM/dd/yyyy\"},\n",
        "                          \"SERVICER\":        {\"dtype\": \"string\", \"drop\":True},\n",
        "                          \"LAST_RT\":         {\"dtype\": \"float\"},\n",
        "                          \"LAST_UPB\":        {\"dtype\": \"double\"},\n",
        "                          \"LOAN_AGE\":        {\"dtype\": \"int\"},\n",
        "                          \"Months_To_Legal_Mat\": {\"dtype\": \"int\", \"default\": -1},\n",
        "                          \"Adj_Month_To_Mat\": {\"dtype\": \"int\", \"default\": -1},\n",
        "                          \"Maturity_Date\":   {\"dtype\": \"date\",                      \"format2\":\"MM/yyyy\"},\n",
        "                          \"MSA\":             {\"dtype\": \"string\", \"drop\":True},\n",
        "                          \"DLQ_STATUS\":      {\"dtype\": \"string\"},\n",
        "                          \"MOD_FLAG\":        {\"dtype\": \"string\", \"drop\":True},\n",
        "                          \"ZB_CODE\":         {\"dtype\": \"string\", \"drop\":True},\n",
        "                          \"ZB_DTE\":          {\"dtype\": \"date\",   \"drop\":True,        \"format2\":\"MM/yyyy\"    },\n",
        "                          \"LPI_DTE\":         {\"dtype\": \"date\",   \"drop\":True,        \"format2\":\"MM/dd/yyyy\" },\n",
        "                          \"FCC_DTE\":         {\"dtype\": \"date\",   \"drop\":True,        \"format2\":\"MM/dd/yyyy\" },\n",
        "                          \"DISP_DTE\":         {\"dtype\": \"date\",   \"drop\":True,        \"format2\":\"MM/dd/yyyy\" },\n",
        "                          \"FCC_COST\":        {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"PP_COST\":         {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"AR_COST\":         {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"IE_COST\":         {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"TAX_COST\":        {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"NS_PROCS\":        {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"CE_PROCS\":        {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"RMW_PROCS\":       {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"O_PROCS\":         {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"NON_INT_UPB\":     {\"dtype\": \"float\"},\n",
        "                          \"PRIN_FORG_UPB_FHFA\": {\"dtype\": \"float\"},\n",
        "                          \"REPCH_FLAG\":      {\"dtype\": \"string\"},\n",
        "                          \"PRIN_FORG_UPB_OTH\": {\"dtype\": \"string\"},\n",
        "                          \"TRANSFER_FLG\":    {\"dtype\": \"string\"},\n",
        "                          }\n",
        "    _AcquisitionSchema_FRED = { \"CSCORE_B\":        {\"dtype\": \"float\", \"default\": -1},\n",
        "                               \"FRST_DTE\":        {\"dtype\": \"date\", \"format\":\"%Y/%m\", \"format2\":\"yyyyMM\"},\n",
        "                              \"FTHB_FLG\":        {\"dtype\": \"string\"},\n",
        "                              \"MAT_DTE\" :        {\"dtype\": \"date\", \"format\":\"%Y/%m\", \"format2\":\"yyyyMM\"},\n",
        "                              \"MSA\":             {\"dtype\": \"string\", \"drop\":True},\n",
        "                              \"MI_PCT\":          {\"dtype\": \"int\", \"default\": 0},\n",
        "                              \"NUM_UNIT\":        {\"dtype\": \"int\", \"default\": -1},\n",
        "                               \"OCC_STAT\":        {\"dtype\": \"string\"},\n",
        "                              \"OCLTV\":           {\"dtype\": \"float\", \"default\": 0},\n",
        "                              \"DTI\":             {\"dtype\": \"int\", \"default\": -1},\n",
        "                              \"ORIG_AMT\":        {\"dtype\": \"double\"},\n",
        "                              \"OLTV\":            {\"dtype\": \"float\", \"default\": 0},\n",
        "                              \"ORIG_RT\":         {\"dtype\": \"float\"},\n",
        "                              \"ORIG_CHN\":        {\"dtype\": \"string\"},\n",
        "                              \"ppmt_pnlty\" :        {\"dtype\": \"string\"},\n",
        "                              \"Product_Type\":    {\"dtype\": \"string\"},\n",
        "                              \"STATE\":           {\"dtype\": \"string\"},\n",
        "                              \"PROP_TYP\":        {\"dtype\": \"string\"},\n",
        "                              \"ZIP_3\":           {\"dtype\": \"string\"},\n",
        "                              \"LOAN_ID\":         {\"dtype\": \"string\"},\n",
        "                              \"PURPOSE\":         {\"dtype\": \"string\"},\n",
        "                              \"ORIG_TRM\":        {\"dtype\": \"int\" },\n",
        "                              \"NUM_BO\":          {\"dtype\": \"int\", \"default\": -1},\n",
        "                              \"SellerName\":      {\"dtype\": \"string\", \"drop\":True},\n",
        "                              \"SERVICER\":        {\"dtype\": \"string\", \"drop\":True},\n",
        "                              \"flag_sc\":        {\"dtype\": \"string\", \"drop\":True}\n",
        "                          }\n",
        "\n",
        "    # schema of Performance Data\n",
        "    _PerformanceSchema_FRED = {\"LOAN_ID\":         {\"dtype\": \"string\"},\n",
        "                          \"ACT_DTE\":         {\"dtype\": \"date\", \"format\":\"%Y/%m/%d\", \"format2\":\"yyyyMM\"},                       \n",
        "                          \"LAST_UPB\":        {\"dtype\": \"double\"},\n",
        "                          \"DLQ_STATUS\":      {\"dtype\": \"string\"},\n",
        "                          \"LOAN_AGE\":        {\"dtype\": \"int\"},\n",
        "                          \"Months_To_Legal_Mat\": {\"dtype\": \"int\", \"default\": -1},\n",
        "                          \"REPCH_FLAG\":      {\"dtype\": \"string\"},\n",
        "                          \"MOD_FLAG\":        {\"dtype\": \"string\", \"drop\":True},\n",
        "                          \"ZB_CODE\":         {\"dtype\": \"string\", \"drop\":True},\n",
        "                          \"ZB_DTE\":          {\"dtype\": \"date\",   \"drop\":True,        \"format2\":\"yyyyMM\"    },\n",
        "                          \"LAST_RT\":         {\"dtype\": \"float\"},\n",
        "                          \"NON_INT_UPB\":     {\"dtype\": \"float\"},\n",
        "                          \"LPI_DTE\":         {\"dtype\": \"date\",   \"drop\":True,        \"format2\":\"yyyyMM\" },\n",
        "                          \"CE_PROCS\":        {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"NS_PROCS\":        {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"RMW_PROCS\":       {\"dtype\": \"float\", \"drop\":True},  \n",
        "                          \"EXpenses\":        {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"LEGAL_COST\":       {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"MAINT_COST\":       {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"TAX_COST\":        {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"MISC_COST\":       {\"dtype\": \"float\", \"drop\":True},\n",
        "                          \"ACTUAL_LOSS\":     {\"dtype\": \"float\", \"drop\":False},\n",
        "                          \"MOD_LOSS\":        {\"dtype\": \"float\", \"drop\":False},\n",
        "                          \"STEPMOD_IND\":     {\"dtype\": \"string\", \"drop\":True},\n",
        "                          \"DPM_IND\":         {\"dtype\": \"string\", \"drop\":True},\n",
        "                          \"ELTV\"   :         {\"dtype\": \"float\", \"drop\":True}\n",
        "                          }\n",
        "    def __init__(self, agency=\"FNMA\"):\n",
        "        self.agency = agency\n",
        "   \n",
        "    def AcquisitionSchema(self):\n",
        "       if self.agency == \"FNMA\":\n",
        "          return self._AcquisitionSchema_FNMA;\n",
        "       else:\n",
        "          return self._AcquisitionSchema_FRED;\n",
        "\n",
        "    def PerformanceSchema(self):\n",
        "       if  self.agency == \"FNMA\":\n",
        "          return self._PerformanceSchema_FNMA;\n",
        "       else:\n",
        "          return self._PerformanceSchema_FRED;\n",
        "\n",
        "    \n",
        "\n",
        "    @staticmethod\n",
        "    def columnType(v):\n",
        "        vout = str\n",
        "        dtype = v.get('dtype')\n",
        "        if dtype == \"float\":\n",
        "            vout = np.float32\n",
        "        elif dtype == \"double\":\n",
        "            vout = np.float64\n",
        "        elif dtype  == \"int\":\n",
        "            value = v.get('default')\n",
        "            if value is None:\n",
        "                vout = np.int32\n",
        "            else:\n",
        "                vout = np.float32\n",
        "        elif dtype  == \"string\":\n",
        "            vout = str\n",
        "        elif dtype == \"date\":\n",
        "            vout = \"date\"\n",
        "        else:\n",
        "            vout = \"other\"\n",
        "        return vout\n",
        "\n",
        "    @staticmethod\n",
        "    def columnType_spark(v):\n",
        "          vout = StringType()\n",
        "          dtype = v.get('dtype')\n",
        "          if dtype == \"float\":\n",
        "             vout = FloatType()\n",
        "          elif dtype == \"double\":\n",
        "            vout = DoubleType()\n",
        "          elif dtype  == \"int\":\n",
        "            vout = IntegerType()\n",
        "          elif dtype  == \"string\":\n",
        "            vout = StringType()\n",
        "          elif dtype == \"date\":\n",
        "            vout = StringType()\n",
        "          else:\n",
        "            vout = StringType()\n",
        "          return vout\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zSZwhWcUQIz",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 class Agency_Loan\n",
        " This is the base class for several other versions defined later. For this base version, pandas package is used for the implementation.\n",
        "\n",
        "Common python libraries (numpy, pandas, sklearn) is widely used by data scientists and analysts for performing data science tasks and these are easy to understand and implement. These libraries are not scalable and work on a single CPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xneo-xYuiUb3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### class implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2RNeIPZZ7N7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sqlalchemy as db\n",
        "\n",
        "class Agency_Loan(object):\n",
        "    '''processing public loan data from Fannie Mae and FRED'''\n",
        "    def __init__(self, agency, acqYYYYQQ, stageFolder=None,  acquisition_file=None, performance_file=None):\n",
        "        self.Data_Schema = Data_Schema(agency)\n",
        "        self.acqYYYYQQ = acqYYYYQQ\n",
        "        self.acquisition_file = acquisition_file\n",
        "        self.performance_file = performance_file\n",
        "        self.stageFolder = stageFolder\n",
        "        self._Loan_Data = None\n",
        "        self._Performance_Data = None\n",
        "\n",
        "    @property\n",
        "    def Loan_Data(self):\n",
        "        return self._Loan_Data\n",
        "\n",
        "    @Loan_Data.setter\n",
        "    def Loan_Data(self, x):\n",
        "        self._Loan_Data = x\n",
        "\n",
        "    @property\n",
        "    def Performance_Data(self):\n",
        "        return self._Performance_Data\n",
        "\n",
        "    @Performance_Data.setter\n",
        "    def Performance_Data(self, x):\n",
        "        self._Performance_Data = x\n",
        "\n",
        "    def whoami(self):\n",
        "        print (type(self).__name__)\n",
        "\n",
        "    @decorator_time\n",
        "    def read_data_acquisition(self, acquisition_file=None, save_as_parquet_file=None):\n",
        "        '''\n",
        "        read and pre-process acqusition data\n",
        "        '''\n",
        "        if acquisition_file is not None:\n",
        "            self.acquisition_file = acquisition_file\n",
        "\n",
        "        col_names = [ k for k in self.Data_Schema.AcquisitionSchema().keys()]\n",
        "        col_dtype = { k: Data_Schema.columnType(v) for k, v in self.Data_Schema.AcquisitionSchema().items() \\\n",
        "                         if Data_Schema.columnType(v) not in ( \"other\", \"date\") }\n",
        "\n",
        "        parse_dates =[ k for k, v in self.Data_Schema.AcquisitionSchema().items() \\\n",
        "                         if Data_Schema.columnType(v) == \"date\" ]\n",
        "\n",
        "        df = pd.read_csv(self.acquisition_file, delimiter ='|', header=None,\n",
        "                    names=col_names,\n",
        "                    dtype=col_dtype,  parse_dates = parse_dates\n",
        "                    )\n",
        "        df['OCLTV'] = df['OCLTV'].fillna(df['OLTV'])\n",
        "        self.Loan_Data = df\n",
        "        print(f\"{self.__class__.__name__}=>Loan_Data shape = ({self.Loan_Data.shape})\")\n",
        "        self.save_as_parquet(Loan_Data_target= save_as_parquet_file, Performance_Data_target = None)\n",
        "        \n",
        "        return None\n",
        "\n",
        "    @decorator_time\n",
        "    def read_data_performance(self, performance_file=None, save_as_parquet_file=None):\n",
        "        '''\n",
        "        read and pre-process performance data\n",
        "        '''\n",
        "        if performance_file is not None:\n",
        "            self.performance_file = performance_file\n",
        "\n",
        "        col_names = [k for k in self.Data_Schema.PerformanceSchema().keys()]\n",
        "        col_dtype = {k: Data_Schema.columnType(v) for k, v in self.Data_Schema.PerformanceSchema().items() \\\n",
        "                     if Data_Schema.columnType(v) not in (\"other\", \"date\")}\n",
        "\n",
        "        parse_dates = [k for k, v in self.Data_Schema.PerformanceSchema().items() \\\n",
        "                       if Data_Schema.columnType(v) == \"date\"]\n",
        "\n",
        "        df = pd.read_csv(self.performance_file, delimiter='|', header=None,\n",
        "                         names=col_names,\n",
        "                         dtype=col_dtype,\n",
        "                         parse_dates=parse_dates\n",
        "                         )\n",
        "\n",
        "        for k, v in self.Data_Schema.PerformanceSchema().items():\n",
        "            value = v.get('default')\n",
        "            if value is not None:\n",
        "                if v.get(\"dtype\") == \"int\":\n",
        "                    df[k] = df[k].fillna(value).astype(\"int32\")\n",
        "                else:\n",
        "                    df[k] = df[k].fillna(value)\n",
        "\n",
        "        df[\"DLQ_STATUS\"]  = df[\"DLQ_STATUS\"].fillna(\"-1\")\n",
        "        df[\"DLQ_STATUS\"]  = df[\"DLQ_STATUS\"].mask(df[\"DLQ_STATUS\"] ==\"X\", \"-2\")\n",
        "        df[\"DLQ_STATUS\"]  = df[\"DLQ_STATUS\"].astype(\"int32\")\n",
        "        df[\"DEFT_COST\"]   = df.FCC_COST + df.PP_COST  + df.AR_COST   + df.IE_COST + df.TAX_COST\n",
        "        df[\"DEFT_PROCS\"]  = df.NS_PROCS + df.CE_PROCS + df.RMW_PROCS + df.O_PROCS\n",
        "        \n",
        "        df = df[df[\"LOAN_AGE\"]>=0]\n",
        "        select_col = [k for k, v in self.Data_Schema.PerformanceSchema().items() if v.get(\"drop\", False) == False]\n",
        "        self.Performance_Data = df[select_col]\n",
        "\n",
        "        print(f\"{self.__class__.__name__}=>performance_data shape = ({self.Performance_Data.shape})\")\n",
        "        self.save_as_parquet(Loan_Data_target= None, Performance_Data_target = save_as_parquet_file)\n",
        "        return None\n",
        "\n",
        "      \n",
        "    @decorator_time\n",
        "    def read_data_performance_chunk(self, performance_file=None, **kwargs):\n",
        "        '''\n",
        "        read and pre-process performance data in chunk because the limited memory.\n",
        "        this function will use sqlite db to host the temp data and \n",
        "        **kwargs will alow use to specify the sqlite db file and chunksize\n",
        "                sqlitefile = performance.db, chunksize=10000\n",
        "          \n",
        "\n",
        "        '''\n",
        "        if performance_file is not None:\n",
        "            self.performance_file = performance_file\n",
        "\n",
        "        col_names = [k for k in self.Data_Schema.PerformanceSchema().keys()]\n",
        "        col_dtype = {k: Data_Schema.columnType(v) for k, v in self.Data_Schema.PerformanceSchema().items() \\\n",
        "                     if Data_Schema.columnType(v) not in (\"other\", \"date\")}\n",
        "\n",
        "        parse_dates = [k for k, v in self.Data_Schema.PerformanceSchema().items() \\\n",
        "                       if Data_Schema.columnType(v) == \"date\"]\n",
        "\n",
        "        sqliteFilePath = self.stageFolder + \"/performance.db\"\n",
        "        chunksize = 100000\n",
        "        if kwargs is not None:\n",
        "           for key, value in kwargs.items():\n",
        "               if key == \"sqlitefile\":\n",
        "                  sqliteFilePath = self.stageFolder + \"/\" + value\n",
        "               if key == \"chunksize\":\n",
        "                  chunksize = int(value)\n",
        "\n",
        "        print(f\"use sqlitefile={sqliteFilePath}, chunksize={chunksize}\")\n",
        "        db_engine = db.create_engine('sqlite:///' + sqliteFilePath)\n",
        "        loop = 0\n",
        "        create_it = True\n",
        "       \n",
        "        for df in pd.read_csv(self.performance_file, delimiter='|', header=None,\n",
        "                         chunksize=chunksize, iterator=True,\n",
        "                         names=col_names,\n",
        "                         dtype=col_dtype,\n",
        "                         parse_dates=parse_dates\n",
        "                         ):\n",
        "            loop = loop + 1\n",
        "            #print(loop)\n",
        "            for k, v in self.Data_Schema.PerformanceSchema().items():\n",
        "                value = v.get('default')\n",
        "                if value is not None:\n",
        "                    if v.get(\"dtype\") == \"int\":\n",
        "                       df[k] = df[k].fillna(value).astype(\"int32\")\n",
        "                    else:\n",
        "                       df[k] = df[k].fillna(value)\n",
        "\n",
        "\n",
        "            #print(\"save_sqlite\")\n",
        "            if create_it:\n",
        "               df.to_sql(\"perf\", db_engine, if_exists='replace')\n",
        "               create_it = False\n",
        "            else:\n",
        "               df.to_sql(\"perf\", db_engine, if_exists='append')\n",
        "\n",
        "        ##self.SQLite2Parquet(\"performance9.db\",  \"performance.parquet\")\n",
        "        return sqliteFilePath\n",
        "\n",
        "    def SQLite2Parquet(self, dbFile, parquetFile):\n",
        "        print(\"SQLite2Parquet\")\n",
        "        db_engine = db.create_engine('sqlite:///' + dbFile)\n",
        "        df =  pd.read_sql_query(\"select * from  perf \", db_engine )\n",
        "        #db_engine.close()\n",
        "        print(\"finsing reading\")\n",
        "        df[\"ACQ\"] = self.acqYYYYQQ\n",
        "        df.to_parquet(self.resultFolder + \"\\\\\" + parquetFile, engine='pyarrow', partition_cols=['ACQ'])\n",
        "        return True\n",
        "\n",
        "    def save_as_parquet(self, Loan_Data_target= None, Performance_Data_target=None):\n",
        "       if Loan_Data_target is not None:\n",
        "           try:\n",
        "              self.Loan_Data.to_parquet(Loan_Data_target)\n",
        "              print(f\"saved Loan data to {Loan_Data_target}\")\n",
        "           except Exception as err:\n",
        "                  print(\"save_as_parquet: Error:  {0}\".format(err))\n",
        "\n",
        "       if Performance_Data_target is not None:\n",
        "           try:\n",
        "              self.Performance_Data.to_parquet(Performance_Data_target)\n",
        "              print(f\"saved Loan data to {Performance_Data_target}\")\n",
        "           except Exception as err:\n",
        "                  print(\"save_as_parquet: Error:  {0}\".format(err))\n",
        "\n",
        "        \n",
        "    def save_as_parquet2(self, resultFolder= None, Loan_Data =True, Performance_Data=True):\n",
        "       if resultFolder is not None:\n",
        "            if os.path.isdir(resultFolder):\n",
        "              try:\n",
        "                 if (Loan_Data == True) and (self.Loan_Data is not None):\n",
        "                   targetFile = resultFolder + \"/\" + self.Data_Schema.agency +  \"/Loan.parquet/ACQ=\" + self.acqYYYYQQ\n",
        "                   self.Loan_Data.write.mode('overwrite').parquet(targetFile)\n",
        "                   print(f\"saved Loan data to {targetFile}\")\n",
        "                 if (Performance_Data == True) and (self.Performance_Data is not None):\n",
        "                   targetFile = resultFolder +  \"/\" + self.Data_Schema.agency +  \"/LoanPerformance.parquet/ACQ=\" + self.acqYYYYQQ\n",
        "                   self.Performance_Data.write.mode('overwrite').parquet(targetFile)\n",
        "                   print(f\"saved Loan Performance data to {targetFile}\")\n",
        "              except Exception as err:\n",
        "                  print(\"save_as_parquet: Error:  {0}\".format(err))\n",
        "\n",
        "    def clear_data(self):\n",
        "       if self._Loan_Data is not None:\n",
        "         del self._Loan_Data\n",
        "         self._Loan_Data = None\n",
        "       if self. _Performance_Data is not None:\n",
        "         del self._Performance_Data\n",
        "         self._Performance_Data = None\n",
        "       \n",
        "    def compute_schd_upb(self, monthCount, outAsMatrix=True, loan_Pandas_Dataframe=None):\n",
        "          '''\n",
        "          Calculate the scheduled UPB based on the Loan_Data[\"ORIG_AMT\", \"ORIG_RT\", \"ORIG_TRM\"]\n",
        "\n",
        "          Parameters\n",
        "          ----------\n",
        "          monthCount: int, sepcify the period to get UPB.\n",
        "          outAsMatrix : default to be True, returning \"matrix\" or  \"pandas_df\"\n",
        "       \n",
        "          Returns\n",
        "          -------\n",
        "          out : ndarray (loanCount, monthCount), pandas dataframe\n",
        "          '''\n",
        "          if loan_Pandas_Dataframe  is None:\n",
        "             sub_df = self._Loan_Data\n",
        "          else:\n",
        "             sub_df = loan_Pandas_Dataframe\n",
        "         \n",
        "          upb_matrix = compute_amortization(principals    = sub_df[\"ORIG_AMT\"], \n",
        "                                            monthly_rates = sub_df[\"ORIG_RT\"] / 1200,\n",
        "                                            terms         = sub_df[\"ORIG_TRM\"],\n",
        "                                            start_period  = 0, \n",
        "                                            end_period    = monthCount)\n",
        "          \n",
        "          if outAsMatrix == True:\n",
        "             return upb_matrix\n",
        "          else:\n",
        "             upb_array = upb_matrix.flatten()\n",
        "             r,c = upb_matrix.shape\n",
        "             ages_value = np.arange(c)\n",
        "             ages_value = np.tile(ages_value, r)\n",
        "             loanids = np.repeat(sub_df[\"LOAN_ID\"].values, c)\n",
        "             upb_array = pd.DataFrame({\"LOAN_ID\": loanids,\n",
        "                           \"LOAN_AGE\": ages_value,\n",
        "                           \"SCHD_UPB\": upb_array})\n",
        "             \n",
        "             return (upb_array)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kJmCScvUrdI",
        "colab_type": "text"
      },
      "source": [
        "### Unit test of class Agency_Loan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMLejp5SUDxl",
        "colab_type": "code",
        "outputId": "42661e37-adc0-4ee6-9f72-8e23cb6f1de2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "#Case 1, FNMA loan\n",
        "myobj = Agency_Loan(agency=\"FNMA\", \n",
        "                    acqYYYYQQ = \"2000Q1\", \n",
        "                    stageFolder = \"/content/drive/My Drive/ML_Data/stock\",\n",
        "                    acquisition_file = \"/content/drive/My Drive/ML_Data/stock/Acquisition_2000Q1.txt\",\n",
        "                    performance_file = \"/content/drive/My Drive/ML_Data/stock/Performance_2000Q1.txt\" )\n",
        "\n",
        "%time myobj.read_data_acquisition(save_as_parquet_file =\"pandas_LL_test.parquet\")\n",
        "%time myobj.read_data_performance(save_as_parquet_file =\"pandas_LP_test.parquet\")\n",
        "myobj.clear_data()\n",
        "%time myobj.read_data_acquisition(save_as_parquet_file =None)\n",
        "%time myobj.read_data_performance(save_as_parquet_file =None)\n",
        "myobj.clear_data()\n",
        "\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agency_Loan=>Loan_Data shape = ((246863, 25))\n",
            "saved Loan data to pandas_LL_test.parquet\n",
            "'read_data_acquisition'  909.26 ms\n",
            "CPU times: user 878 ms, sys: 36.1 ms, total: 914 ms\n",
            "Wall time: 909 ms\n",
            "Agency_Loan=>performance_data shape = ((9128092, 14))\n",
            "saved Loan data to pandas_LP_test.parquet\n",
            "'read_data_performance'  30465.28 ms\n",
            "CPU times: user 29.6 s, sys: 787 ms, total: 30.4 s\n",
            "Wall time: 30.5 s\n",
            "Agency_Loan=>Loan_Data shape = ((246863, 25))\n",
            "'read_data_acquisition'  636.44 ms\n",
            "CPU times: user 596 ms, sys: 17 ms, total: 613 ms\n",
            "Wall time: 637 ms\n",
            "Agency_Loan=>performance_data shape = ((9128092, 14))\n",
            "'read_data_performance'  26894.36 ms\n",
            "CPU times: user 25.9 s, sys: 514 ms, total: 26.4 s\n",
            "Wall time: 26.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciN1zV92bsUr",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 class Agency_Loan_Dask\n",
        "In this implementation, we will use **Dask** package to overcome the limitation of pandas. **Dask** can efficiently perform parallel computations on a single machine using multi-core CPUs. For example, if you have a quad core processor, Dask can effectively use all 4 cores of your system simultaneously for processing. Dask supports the Pandas dataframe and Numpy array data structures to analyze large datasets.\n",
        "\n",
        "Key Features of Dask:\n",
        "* Multiple numpy arrays are grouped together to form a Dask array. \n",
        "* A Dask dataframe consists of multiple smaller pandas dataframes. A large pandas dataframe splits row-wise to form multiple smaller dataframes. These smaller dataframes are present on a disk of a single machine, or multiple machines (thus allowing to store datasets of size larger than the memory). \n",
        "* Each computation on a Dask dataframe parallelizes operations on the existing pandas dataframes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvtfv0JpORiV",
        "colab_type": "code",
        "outputId": "d5b29239-c27a-4453-b6e3-1f8b1405d337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#install dask,  do this step if needed.\n",
        "!pip install \"dask\"\n",
        "#or\n",
        "#!pip install \"dask[complete]\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dask in /usr/local/lib/python3.6/dist-packages (2.9.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGavPChdwh9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dask.dataframe as dd\n",
        "\n",
        "class Agency_Loan_Dask(Agency_Loan):\n",
        "    '''processing public loan data from Fannie Mae '''\n",
        "      #schema of Acquisition Data\n",
        "    \n",
        "    def __init__(self, agency, acqYYYYQQ, stageFolder=None,  acquisition_file=None, performance_file=None):\n",
        "         super().__init__(agency, acqYYYYQQ, stageFolder    , acquisition_file, performance_file)\n",
        "\n",
        "    def whoami(self):\n",
        "        print (type(self).__name__)\n",
        "\n",
        "    @decorator_time\n",
        "    def read_data_acquisition(self, acquisition_file=None, save_as_parquet_file=None):\n",
        "        '''\n",
        "        read and pre-process acqusition data\n",
        "        '''\n",
        "        if acquisition_file is not None:\n",
        "            self.acquisition_file = acquisition_file\n",
        "\n",
        "        col_names = [ k for k in self.Data_Schema.AcquisitionSchema().keys()]\n",
        "        col_dtype = { k: Data_Schema.columnType(v) for k, v in self.Data_Schema.AcquisitionSchema().items() \\\n",
        "                         if Data_Schema.columnType(v) not in ( \"other\", \"date\") }\n",
        "\n",
        "        parse_dates =[ k for k, v in self.Data_Schema.AcquisitionSchema().items() \\\n",
        "                         if Data_Schema.columnType(v) == \"date\" ]\n",
        "\n",
        "        #print(parse_dates)\n",
        "\n",
        "        df = dd.read_csv(self.acquisition_file, delimiter ='|', header=None,\n",
        "                    names=col_names,\n",
        "                    dtype=col_dtype,  parse_dates = parse_dates\n",
        "                    )\n",
        "        for k, v in self.Data_Schema.AcquisitionSchema().items():\n",
        "            value = v.get('default')\n",
        "            if value is not None:\n",
        "                if v.get(\"dtype\") == \"int\":\n",
        "                    df[k] = df[k].fillna(value).astype(\"int32\")\n",
        "                else:\n",
        "                    df[k] = df[k].fillna(value)\n",
        "\n",
        "        df['OCLTV'] = df['OCLTV'].fillna(df['OLTV'])\n",
        "        for k, v in self.Data_Schema.AcquisitionSchema().items():\n",
        "             if v.get('drop') == True:\n",
        "                df= df.drop(k, axis=1)\n",
        "        #df.compute()\n",
        "        self.Loan_Data = df\n",
        "       \n",
        "        #print(f\"{self.__class__.__name__}=>Loan_Data shape = ({len(self.Loan_Data), len(self.Loan_Data.columns)})\")\n",
        "        self.save_as_parquet(Loan_Data_target= save_as_parquet_file, Performance_Data_target = None)\n",
        "\n",
        "        return None\n",
        "\n",
        "    @decorator_time\n",
        "    def read_data_performance(self, performance_file=None, save_as_parquet_file=None):\n",
        "        '''\n",
        "        read and pre-process performance data\n",
        "        '''\n",
        "        if performance_file is not None:\n",
        "            self.performance_file = performance_file\n",
        "\n",
        "        col_names = [k for k in self.Data_Schema.PerformanceSchema().keys()]\n",
        "        col_dtype = {k: Data_Schema.columnType(v) for k, v in self.Data_Schema.PerformanceSchema().items() \\\n",
        "                     if Data_Schema.columnType(v) not in (\"other\", \"date\")}\n",
        "\n",
        "        parse_dates = [k for k, v in self.Data_Schema.PerformanceSchema().items() \\\n",
        "                       if Data_Schema.columnType(v) == \"date\"]\n",
        "\n",
        "        df = dd.read_csv(self.performance_file, delimiter='|', header=None,\n",
        "                         names=col_names,\n",
        "                         dtype=col_dtype,\n",
        "                         parse_dates=parse_dates\n",
        "                         )\n",
        "\n",
        "        for k, v in self.Data_Schema.PerformanceSchema().items():\n",
        "            value = v.get('default')\n",
        "            if value is not None:\n",
        "                if v.get(\"dtype\") == \"int\":\n",
        "                    df[k] = df[k].fillna(value).astype(\"int32\")\n",
        "                else:\n",
        "                    df[k] = df[k].fillna(value)\n",
        "\n",
        "     \n",
        "        df[\"DLQ_STATUS\"]  = df[\"DLQ_STATUS\"].fillna(\"-1\")\n",
        "        df[\"DLQ_STATUS\"]  = df[\"DLQ_STATUS\"].mask(df[\"DLQ_STATUS\"] ==\"X\", \"-2\")\n",
        "        df[\"DLQ_STATUS\"]  = df[\"DLQ_STATUS\"].astype(\"int32\")\n",
        "        df[\"DEFT_COST\"]   = df.FCC_COST + df.PP_COST  + df.AR_COST   + df.IE_COST + df.TAX_COST\n",
        "        df[\"DEFT_PROCS\"]  = df.NS_PROCS + df.CE_PROCS + df.RMW_PROCS + df.O_PROCS\n",
        "        select_col = [k for k, v in self.Data_Schema.PerformanceSchema().items() if v.get(\"drop\", False) == False]\n",
        "\n",
        "        self.Performance_Data = df[select_col]\n",
        "        #print(f\"{self.__class__.__name__}=>Performance_Data shape = ({len(self.Performance_Data), len(self.Performance_Data.columns)})\")\n",
        "        self.save_as_parquet(Loan_Data_target= None, Performance_Data_target = save_as_parquet_file)\n",
        "        return None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbWQJfqY26xi",
        "colab_type": "code",
        "outputId": "fd480959-1dc4-4349-d8a7-831a6fd5056e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "myobj2 = Agency_Loan_Dask(agency=\"FNMA\", \n",
        "                    acqYYYYQQ = \"2000Q1\", \n",
        "                    stageFolder = \"/content/drive/My Drive/ML_Data/stock\",\n",
        "                    acquisition_file = \"/content/drive/My Drive/ML_Data/stock/Acquisition_2000Q1.txt\",\n",
        "                    performance_file = \"/content/drive/My Drive/ML_Data/stock/Performance_2000Q1.txt\" )\n",
        "\n",
        "%time myobj2.read_data_acquisition()\n",
        "%time myobj2.read_data_acquisition(save_as_parquet_file =\"dask_LL_test.parquet\")\n",
        "%time myobj2.read_data_performance()\n",
        "%time myobj2.read_data_performance(save_as_parquet_file =\"dask_PL_test.parquet\")\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'read_data_acquisition'  144.06 ms\n",
            "CPU times: user 139 ms, sys: 2.12 ms, total: 141 ms\n",
            "Wall time: 144 ms\n",
            "saved Loan data to dask_LL_test.parquet\n",
            "'read_data_acquisition'  1510.64 ms\n",
            "CPU times: user 1.45 s, sys: 57.1 ms, total: 1.51 s\n",
            "Wall time: 1.51 s\n",
            "'read_data_performance'  184.69 ms\n",
            "CPU times: user 132 ms, sys: 50.1 ms, total: 182 ms\n",
            "Wall time: 185 ms\n",
            "saved Loan data to dask_PL_test.parquet\n",
            "'read_data_performance'  28185.96 ms\n",
            "CPU times: user 47.3 s, sys: 2.04 s, total: 49.3 s\n",
            "Wall time: 28.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MMX_Wxaqkr5",
        "colab_type": "text"
      },
      "source": [
        "## 2.5 Class Agency_Loan_spark. \n",
        "\n",
        "Re-implementation of  Agency_Loan using **pyspark** API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tm9zTFsd-n1",
        "colab_type": "text"
      },
      "source": [
        "### set up spark environment on Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yvlg8f8eMec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "\n",
        "#package to add PySpark to sys.path at runtime\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init() \n",
        "!cat /proc/cpuinfo\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aJpXIiSfy4p",
        "colab_type": "text"
      },
      "source": [
        "### class implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g09mFAJzyUYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "#cannot import name 'monotonically_increasing_id'\n",
        "from pyspark.sql import functions as F,  Window\n",
        "##from pyspark.sql.functions import col, udf, to_date, when, collect_list, lag,  max, row_number, monotonically_increasing_id\n",
        "from pyspark.sql.types import DateType, StructType, StructField,  DoubleType, FloatType, IntegerType, LongType, StringType, DateType\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class Agency_Loan_spark(Agency_Loan):\n",
        "    '''processing public loan data from Fannie Mae '''\n",
        "      #schema of Acquisition Data\n",
        "    \n",
        "    def __init__(self, agency, acqYYYYQQ, stageFolder=None,  acquisition_file=None, performance_file=None):\n",
        "         super().__init__(agency, acqYYYYQQ, stageFolder    , acquisition_file, performance_file)\n",
        "         \n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def save_to_hive(dbname, tablename, dataframe_src):\n",
        "          #1. Creating Hive Database\n",
        "          spark.sql('create database IF NOT EXISTS ' + dbname)\n",
        "          spark.sql('use ' + dbname )\n",
        "          spark.sql(\"drop table IF  EXISTS \" + tablename)                                   \n",
        "          spark.sql(\"show tables\").show()\n",
        "\n",
        "          spark.sql(\"insert into table ratings \\\n",
        "                     select * from \" +dataframe_src )\n",
        "\n",
        "    def compute_schd_upb(self, monthCount, outAsMatrix=True):\n",
        "          '''\n",
        "          Calculate the scheduled UPB based on the Loan_Data[\"ORIG_AMT\", \"ORIG_RT\", \"ORIG_TRM\"]\n",
        "\n",
        "          Parameters\n",
        "          ----------\n",
        "          monthCount: int, sepcify the period to get UPB.\n",
        "          outformat : string, \"matrix\", \"pandas_df\", \"spark_df\"\n",
        "       \n",
        "          Returns\n",
        "          -------\n",
        "          out : ndarray (M, N),  spark dataframe\n",
        "          '''\n",
        "          sub_df = self.Loan_Data.select(\"LOAN_ID\", \"ORIG_AMT\", \"ORIG_RT\", \"ORIG_TRM\").toPandas()\n",
        "         \n",
        "          upb_matrix = super().compute_schd_upb(monthCount, outAsMatrix, sub_df)\n",
        "          \n",
        "          if outAsMatrix == True:\n",
        "             return upb_matrix\n",
        "          else:\n",
        "             return spark.createDataFrame(upb_matrix)\n",
        "        \n",
        "    def save_as_parquet(self, Loan_Data_target= None, Performance_Data_target=None):\n",
        "       if Loan_Data_target is not None:\n",
        "           try:\n",
        "              self.Loan_Data.write.mode('overwrite').parquet(Loan_Data_target)\n",
        "             \n",
        "              print(f\"saved Loan data to {Loan_Data_target}\")\n",
        "           except Exception as err:\n",
        "                  print(\"save_as_parquet: Error:  {0}\".format(err))\n",
        "\n",
        "       if Performance_Data_target is not None:\n",
        "           try:\n",
        "              self.Performance_Data.write.mode('overwrite').parquet(Performance_Data_target)\n",
        "              print(f\"saved Loan data to {Performance_Data_target}\")\n",
        "           except Exception as err:\n",
        "                  print(\"save_as_parquet: Error:  {0}\".format(err))\n",
        "\n",
        "\n",
        "    def read_data_acquisition (self, acquisition_file=None, save_as_parquet_file=None):\n",
        "          '''\n",
        "          read and pre-process acqusition data\n",
        "          '''\n",
        "          if acquisition_file is not None:\n",
        "             self.acquisition_file = acquisition_file\n",
        "\n",
        "          # need to check file existing\n",
        "          ColumnSchema = StructType([StructField(k, Data_Schema.columnType_spark(v), True) for k, v in self.Data_Schema.AcquisitionSchema().items()])                   \n",
        "          ts = time.time()\n",
        "          acq_df = spark.read.format(\"csv\").options(header='False', delimiter=\"|\").schema(ColumnSchema).load(self.acquisition_file )\n",
        "          \n",
        "          for k, v in self.Data_Schema.AcquisitionSchema().items():\n",
        "             if v.get('dtype') == \"date\":\n",
        "                acq_df = acq_df.withColumn(k,  F.to_date(F.col(k), v.get('format2')))\n",
        "             else:\n",
        "                value = v.get('default')\n",
        "                if value is not None:\n",
        "                   acq_df = acq_df.withColumn(k, F.when(F.col(k).isNull(), value).otherwise(F.col(k)))\n",
        "\n",
        "          acq_df = acq_df.withColumn('OCLTV',     F.expr(\"case when OCLTV ==0 then OLTV else OCLTV end\"))\n",
        "\n",
        "          for k, v in self.Data_Schema.AcquisitionSchema().items():\n",
        "             if v.get('drop') == True:\n",
        "                acq_df= acq_df.drop(k)\n",
        "         \n",
        "          self.Loan_Data = acq_df\n",
        "          #print(f\"{ self.__class__.__name__ }=> Loan_Data shape = ({self.Loan_Data.count()}, {len(self.Loan_Data.columns)})\")\n",
        "          self.save_as_parquet(Loan_Data_target= save_as_parquet_file, Performance_Data_target = None)\n",
        "          return None\n",
        "\n",
        "          save_as_parquet_file=None\n",
        "    def read_data_performance (self, performance_file=None, save_as_parquet_file=None):\n",
        "          '''\n",
        "          read and pre-process performance data\n",
        "          '''\n",
        "          if performance_file is not None:\n",
        "             self.performance_file = performance_file\n",
        "\n",
        "          # need to check file existing\n",
        "          ColumnSchema = [StructField(k, Data_Schema.columnType_spark(v), True) for k, v in self.Data_Schema.PerformanceSchema().items()]\n",
        "          ColumnSchema = StructType(ColumnSchema)                                \n",
        "\n",
        "          per_df = spark.read.format(\"csv\").options(header='False', delimiter=\"|\").schema(ColumnSchema).load(self.performance_file )\n",
        "          for k, v in self.Data_Schema.PerformanceSchema().items():\n",
        "             if v.get('dtype') == \"date\":\n",
        "                per_df = per_df.withColumn(k,  F.to_date(F.col(k), v.get('format2')))\n",
        "\n",
        "          per_df = per_df.withColumn('DLQ_STATUS', F.when(F.col('DLQ_STATUS').isNull(), -1).when(F.col('DLQ_STATUS') == \"X\", -2).otherwise(F.col('DLQ_STATUS').cast(IntegerType())))\n",
        "          per_df = per_df.withColumn('DEFT_COST',  F.expr(\"FCC_COST + PP_COST + AR_COST + IE_COST + TAX_COST\"))\n",
        "          per_df = per_df.withColumn('DEFT_PROCS',  F.expr(\"NS_PROCS + CE_PROCS + RMW_PROCS + O_PROCS\"))\n",
        "\n",
        "          select_col = [k for k, v in self.Data_Schema.PerformanceSchema().items() if v.get(\"drop\", False) == False]\n",
        "          per_df = per_df.select(select_col)\n",
        "\n",
        "          self.Performance_Data  =per_df.filter(F.col(\"LOAN_AGE\")>=0)\n",
        "          #print(f\"{self.__class__.__name__}=>performance_data shape = ({self.Performance_Data.count()}, {len(self.Performance_Data.columns)})\")\n",
        "          self.save_as_parquet(Loan_Data_target= None, Performance_Data_target = save_as_parquet_file)\n",
        "\n",
        "          return None\n",
        "\n",
        "    def read_data_performance2 (self, performance_file=None):\n",
        "          '''\n",
        "          version 2. read and pre-process performance data, containing more complicated logic and processing.\n",
        "          '''\n",
        "          if performance_file is not None:\n",
        "             self.performance_file = performance_file\n",
        "\n",
        "          # need to check file existing\n",
        "          ColumnSchema = [StructField(k, Data_Schema.columnType_spark(v), True) for k, v in self.Data_Schema.PerformanceSchema().items()]\n",
        "          ColumnSchema = StructType(ColumnSchema)                                \n",
        "\n",
        "          per_df = spark.read.format(\"csv\").options(header='False', delimiter=\"|\").schema(ColumnSchema).load(self.performance_file )\n",
        "          for k, v in self.Data_Schema.PerformanceSchema().items():\n",
        "             if v.get('dtype') == \"date\":\n",
        "                per_df = per_df.withColumn(k,  F.to_date(F.col(k), v.get('format2')))\n",
        "\n",
        "          per_df = per_df.withColumn('DLQ_STATUS', F.when(F.col('DLQ_STATUS').isNull(), -1).when(F.col('DLQ_STATUS') == \"X\", -2).otherwise(F.col('DLQ_STATUS').cast(IntegerType())))\n",
        "          per_df = per_df.withColumn('DEFT_COST',  F.expr(\"FCC_COST + PP_COST + AR_COST + IE_COST + TAX_COST\"))\n",
        "          per_df = per_df.withColumn('DEFT_PROCS',  F.expr(\"NS_PROCS + CE_PROCS + RMW_PROCS + O_PROCS\"))\n",
        "\n",
        "          #udf\n",
        "          #str_to_date =  udf (lambda x: datetime.strptime(x, '%m/%Y'), DateType())   \n",
        "          windowIncF12 =  Window().partitionBy(\"LOAN_ID\").orderBy(\"ACT_DTE\").rowsBetween(1, 12)\n",
        "          windowIncF24 =  Window().partitionBy(\"LOAN_ID\").orderBy(\"ACT_DTE\").rowsBetween(1, 24)\n",
        "          windowIncP12 =  Window().partitionBy(\"LOAN_ID\").orderBy(\"ACT_DTE\").rowsBetween(-12, -1)\n",
        "          windowInc    =  Window().partitionBy(\"LOAN_ID\").orderBy(\"ACT_DTE\")\n",
        "\n",
        "          per_df = per_df.withColumn(\"DLQ_LAG\", F.lag('DLQ_STATUS', 1).over(windowInc)) \\\n",
        "              .withColumn(\"DLQ_LAGs12\",         F.collect_list('DLQ_STATUS').over(windowIncP12)) \\\n",
        "              .withColumn(\"DLQ_NEXT12MAX\",      F.max('DLQ_STATUS').over(windowIncF12)) \\\n",
        "              .withColumn(\"DLQ_NEXT24MAX\",      F.max('DLQ_STATUS').over(windowIncF24)) \\\n",
        "              .withColumn(\"ZBCODE_NEXT12\",      F.max('ZB_CODE').over(windowIncF12)) \\\n",
        "              .withColumn(\"ZBCODE_NEXT24\",      F.max('ZB_CODE').over(windowIncF24)) \\\n",
        "              .withColumn(\"MOD_NEXT12\",         F.max('MOD_FLAG').over(windowIncF12)) \\\n",
        "              .withColumn(\"MOD_NEXT24\",         F.max('MOD_FLAG').over(windowIncF24)) \n",
        "\n",
        "          print(per_df.printSchema())\n",
        "        \n",
        "          ts = time.time()\n",
        "          Mod_DF = per_df.filter(F.col(\"MOD_FLAG\")=='Y').withColumn(\"rn\", F.row_number().over(windowInc)).where((F.col(\"rn\") ==1)) \\\n",
        "                      .select(\"LOAN_ID\", \n",
        "                              F.col(\"ACT_DTE\").alias(\"MOD_DTE\"), \n",
        "                              F.col(\"LOAN_AGE\").alias(\"MOD_AGE\"), \n",
        "                              F.col(\"DLQ_STATUS\").alias(\"MOD_DLQ\"),\n",
        "                              F.col(\"DLQ_LAG\").alias(\"MOD_DLQ_LAG\"),\n",
        "                              F.col(\"DLQ_LAGs12\").alias(\"MOD_DLQ_LAGs12\"),\n",
        "                              F.col(\"DLQ_NEXT12MAX\").alias(\"MOD_POST_MAXDLQ_12\"),\n",
        "                              F.col(\"DLQ_NEXT24MAX\").alias(\"MOD_POST_MAXDLQ_24\"),\n",
        "                              F.col(\"ZBCODE_NEXT12\").alias(\"MOD_POST_ZBCODE_12\"),\n",
        "                              F.col(\"ZBCODE_NEXT24\").alias(\"MOD_POST_ZBCODE_24\")\n",
        "                              )\n",
        "          print(\"Mod DF: \", showtime(ts))\n",
        "        \n",
        "          ts = time.time()\n",
        "          F3Q_DF = per_df.filter(F.col(\"DLQ_STATUS\")>2).withColumn(\"rn\", F.row_number().over(windowInc)).where((F.col(\"rn\") ==1)) \\\n",
        "                     .select(\"LOAN_ID\", \n",
        "                              F.col(\"ACT_DTE\").alias(\"F3Q_DTE\"), \n",
        "                              F.col(\"LOAN_AGE\").alias(\"F3Q_AGE\"), \n",
        "                              F.col(\"DLQ_LAGs12\").alias(\"F3Q_DLQ_LAGs12\"),\n",
        "                              F.col(\"DLQ_NEXT12MAX\").alias(\"F3Q_POST_MAXDLQ_12\"),\n",
        "                              F.col(\"DLQ_NEXT24MAX\").alias(\"F3Q_POST_MAXDLQ_24\"),\n",
        "                              F.col(\"ZBCODE_NEXT12\").alias(\"F3Q_POST_ZBCODE_12\"),\n",
        "                              F.col(\"ZBCODE_NEXT24\").alias(\"F3Q_POST_ZBCODE_24\")\n",
        "                              )\n",
        "          print(\"F3Q DF: \", showtime(ts))\n",
        "        \n",
        "          ts = time.time()\n",
        "          ZB_DF = per_df.filter(F.col(\"ZB_CODE\").isNotNull()) \\\n",
        "                    .select(\"LOAN_ID\", \"ZB_CODE\", \"ZB_DTE\",\n",
        "                              F.col(\"LOAN_AGE\").alias(\"ZB_AGE\"), \n",
        "                              F.col(\"DLQ_LAGs12\").alias(\"ZB_DLQ_LAGs12\"),\n",
        "                              F.col(\"DLQ_LAG\").alias(\"ZB_DLQ_LAG\"),\n",
        "                              F.col(\"LAST_UPB\").alias(\"ZB_LAST_UPB\"),\n",
        "                              \"LPI_DTE\",\n",
        "                              \"FCC_DTE\",\n",
        "                              \"DISP_DTE\", \"DEFT_COST\", \"DEFT_PROCS\"\n",
        "                    )\n",
        "          print(\"ZB DF: \", showtime(ts))\n",
        "        \n",
        "          ts = time.time()\n",
        "\n",
        "          if self.Loan_Data is None:\n",
        "             loanLevelDF =  Mod_DF.select(\"LOAN_ID\") \\\n",
        "                     .union(F3Q_DF.select(\"LOAN_ID\")) \\\n",
        "                     .union( ZB_DF.select(\"LOAN_ID\")).distinct()\n",
        "          \n",
        "             loanLevelDF = loanLevelDF.join(Mod_DF, \"LOAN_ID\", how=\"left\")\n",
        "             loanLevelDF = loanLevelDF.join(F3Q_DF, \"LOAN_ID\", how=\"left\")\n",
        "             loanLevelDF = loanLevelDF.join(ZB_DF,  \"LOAN_ID\",  how=\"left\") \n",
        "          else:\n",
        "             self.Loan_Data = self._Loan_Data.join(Mod_DF, \"LOAN_ID\", how=\"left\") \\\n",
        "                                         .join(F3Q_DF, \"LOAN_ID\", how=\"left\") \\\n",
        "                                         .join(ZB_DF,  \"LOAN_ID\",  how=\"left\").orderBy([\"LOAN_ID\"])\n",
        "              \n",
        "         \n",
        "          select_col = [k for k, v in self.Data_Schema.PerformanceSchema().items() if v.get(\"drop\", False) == False]\n",
        "          Performance_Data = per_df.select(select_col)\n",
        "\n",
        "          #calcualte schd_upb\n",
        "          age_max = 12  #Performance_Data.agg({\"LOAN_AGE\": \"max\"}).first()[0]\n",
        "          ts = time.time()\n",
        "          schd_upbData = self.compute_schd_upb(monthCount=age_max, outAsMatrix=False)\n",
        "          print(\"compute upb: \", showtime(ts))\n",
        "         \n",
        "          ts = time.time()\n",
        "          self.Performance_Data = Performance_Data.join(schd_upbData, on=[\"LOAN_ID\", \"LOAN_AGE\"], how=\"left\").orderBy([\"LOAN_ID\",  \"ACT_DTE\"])\n",
        "          print(\"joining upb: \", showtime(ts))\n",
        "          ts = time.time()\n",
        "          self.Performance_Data = self.Performance_Data.filter(F.col(\"LOAN_AGE\")>=0).withColumn(\"LAST_UPB\",  F.expr(\"case when LAST_UPB is Null then round(SCHD_UPB,3) else LAST_UPB end\") )\n",
        "          self.Performance_Data=  self.Performance_Data.drop(\"SCHD_UPB\")\n",
        "          print(\"filtering out: \", showtime(ts))\n",
        "           \n",
        "          return None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkoN8jd4h73V",
        "colab_type": "text"
      },
      "source": [
        "### unit test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HvG3pwAilX5",
        "colab_type": "code",
        "outputId": "1e32d79a-e543-47b3-85e6-053f6fe97d51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        }
      },
      "source": [
        "import doctest\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField\n",
        "from pyspark.sql.types import DoubleType, LongType, StringType, DateType\n",
        "globs = globals()\n",
        "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "globs['sc'] = sc\n",
        "globs['spark'] = spark\n",
        "   \n",
        "myobj3 = Agency_Loan_spark(agency=\"FNMA\",\n",
        "                            acqYYYYQQ = \"2000Q1\", \n",
        "                            stageFolder = \"/content/drive/My Drive/ML_Data/stock\",\n",
        "                            acquisition_file = \"/content/drive/My Drive/ML_Data/stock/Acquisition_2000Q1.txt\",\n",
        "                            performance_file = \"/content/drive/My Drive/ML_Data/stock/Performance_2000Q1.txt\" )\n",
        "%time myobj3.read_data_acquisition()\n",
        "%time myobj3.read_data_acquisition(save_as_parquet_file =\"spark_LL_test.parquet\")\n",
        "%time myobj3.read_data_performance()\n",
        "%time myobj3.read_data_performance(save_as_parquet_file =\"spark_PL_test.parquet\")\n",
        "\n",
        "myobj3.Loan_Data.show(10)\n",
        "myobj3.Performance_Data.show(10)\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 14.3 ms, sys: 2.95 ms, total: 17.3 ms\n",
            "Wall time: 185 ms\n",
            "saved Loan data to spark_LL_test.parquet\n",
            "CPU times: user 16.4 ms, sys: 903 µs, total: 17.3 ms\n",
            "Wall time: 1.86 s\n",
            "CPU times: user 17.1 ms, sys: 4.79 ms, total: 21.9 ms\n",
            "Wall time: 134 ms\n",
            "saved Loan data to spark_PL_test.parquet\n",
            "CPU times: user 50.7 ms, sys: 4.53 ms, total: 55.2 ms\n",
            "Wall time: 31.4 s\n",
            "+------------+--------+-------+--------+--------+----------+----------+----+-----+------+---+--------+--------+-------+--------+--------+--------+-----+-----+------+------------+--------+-------+--------------+\n",
            "|     LOAN_ID|ORIG_CHN|ORIG_RT|ORIG_AMT|ORIG_TRM|  ORIG_DTE|  FRST_DTE|OLTV|OCLTV|NUM_BO|DTI|CSCORE_B|FTHB_FLG|PURPOSE|PROP_TYP|NUM_UNIT|OCC_STAT|STATE|ZIP_3|MI_PCT|Product_Type|CSCORE_C|MI_TYPE|RELOCATION_FLG|\n",
            "+------------+--------+-------+--------+--------+----------+----------+----+-----+------+---+--------+--------+-------+--------+--------+--------+-----+-----+------+------------+--------+-------+--------------+\n",
            "|100007365142|       R|    8.0| 75000.0|     360|1999-12-01|2000-02-01|79.0| 79.0|     1| 62|   763.0|       N|      R|      SF|       1|       P|   PA|  173|     0|         FRM|      -1|      0|             N|\n",
            "|100007386460|       B|  7.875| 55000.0|     180|2000-01-01|2000-03-01|69.0| 69.0|     1| 12|   633.0|       N|      R|      CO|       1|       P|   MD|  208|     0|         FRM|      -1|      0|             N|\n",
            "|100011322040|       C|   7.75|123000.0|     360|1999-11-01|2000-01-01|80.0| 80.0|     1| 28|   750.0|       N|      P|      SF|       1|       P|   MO|  630|     0|         FRM|      -1|      0|             N|\n",
            "|100015192562|       R|    8.5| 51000.0|     360|2000-02-01|2000-04-01|95.0| 95.0|     1| 27|   686.0|       N|      P|      SF|       1|       P|   GA|  316|    25|         FRM|      -1|      1|             N|\n",
            "|100015874399|       C|   8.75|242000.0|     360|2000-02-01|2000-04-01|95.0| 95.0|     1| 47|   706.0|       N|      P|      SF|       1|       P|   FL|  335|    30|         FRM|      -1|      1|             N|\n",
            "|100017922445|       C|   8.25|240000.0|     360|1999-12-01|2000-02-01|77.0| 77.0|     2| 19|   737.0|       N|      P|      SF|       1|       P|   MI|  483|     0|         FRM|     731|      0|             N|\n",
            "|100019048933|       C|    7.5| 52000.0|     180|1999-12-01|2000-02-01|95.0| 95.0|    -1| -1|    -1.0|       U|      P|      SF|       1|       P|   FL|  347|    30|         FRM|      -1|      1|             N|\n",
            "|100020205696|       R|  7.625|225000.0|     360|1999-07-01|1999-09-01|64.0| 64.0|     2| 21|   793.0|       N|      P|      SF|       1|       P|   WA|  980|     0|         FRM|     770|      0|             N|\n",
            "|100021703104|       C|    8.0|120000.0|     360|2000-02-01|2000-04-01|75.0| 75.0|     2| 36|   750.0|       Y|      P|      SF|       1|       P|   CT|  063|     0|         FRM|      -1|      0|             N|\n",
            "|100023274028|       R|    8.0|130000.0|     360|2000-01-01|2000-03-01|61.0| 61.0|     2| 34|   646.0|       N|      P|      SF|       1|       P|   GA|  300|     0|         FRM|     782|      0|             N|\n",
            "+------------+--------+-------+--------+--------+----------+----------+----+-----+------+---+--------+--------+-------+--------+--------+--------+-----+-----+------+------------+--------+-------+--------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+------------+----------+-------+--------+--------+-------------------+----------------+-------------+----------+-----------+------------------+----------+-----------------+------------+\n",
            "|     LOAN_ID|   ACT_DTE|LAST_RT|LAST_UPB|LOAN_AGE|Months_To_Legal_Mat|Adj_Month_To_Mat|Maturity_Date|DLQ_STATUS|NON_INT_UPB|PRIN_FORG_UPB_FHFA|REPCH_FLAG|PRIN_FORG_UPB_OTH|TRANSFER_FLG|\n",
            "+------------+----------+-------+--------+--------+-------------------+----------------+-------------+----------+-----------+------------------+----------+-----------------+------------+\n",
            "|100007365142|2000-01-01|    8.0|    null|       0|                360|             359|   2030-01-01|         0|       null|              null|      null|             null|        null|\n",
            "|100007365142|2000-02-01|    8.0|    null|       1|                359|             358|   2030-01-01|         0|       null|              null|      null|             null|        null|\n",
            "|100007365142|2000-03-01|    8.0|    null|       2|                358|             357|   2030-01-01|         0|       null|              null|      null|             null|        null|\n",
            "|100007365142|2000-04-01|    8.0|    null|       3|                357|             356|   2030-01-01|         0|       null|              null|      null|             null|        null|\n",
            "|100007365142|2000-05-01|    8.0|    null|       4|                356|             355|   2030-01-01|         0|       null|              null|      null|             null|        null|\n",
            "|100007365142|2000-06-01|    8.0|    null|       5|                355|             355|   2030-01-01|         0|       null|              null|      null|             null|        null|\n",
            "|100007365142|2000-07-01|    8.0| 74693.0|       6|                354|             354|   2030-01-01|         0|       null|              null|      null|             null|        null|\n",
            "|100007365142|2000-08-01|    8.0|74587.91|       7|                353|             352|   2030-01-01|         0|       null|              null|      null|             null|        null|\n",
            "|100007365142|2000-09-01|    8.0|74534.84|       8|                352|             351|   2030-01-01|         0|       null|              null|      null|             null|        null|\n",
            "|100007365142|2000-10-01|    8.0|74481.42|       9|                351|             350|   2030-01-01|         0|       null|              null|      null|             null|        null|\n",
            "+------------+----------+-------+--------+--------+-------------------+----------------+-------------+----------+-----------+------------------+----------+-----------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swMj2t2cvFk1",
        "colab_type": "text"
      },
      "source": [
        "## Summary on Performance Comparison for 3 implementations.\n",
        "\n",
        "In the above, we present 3 implementation of read_loan_data and read_performance_data. They are in **pandas, dask and spark**. Since dask and spark use lazy-evaluation, so we decide to dump data into parquet files.\n",
        "\n",
        "\n",
        "|Task | implementation | duration |\n",
        "|-----|----------------|----------|\n",
        "|acquisition | pandas | 933 ms |\n",
        "|            | dask   | 1.55 s |\n",
        "|            | spark  | 2.1 s       |\n",
        "|            |        |         |\n",
        "|performance |pandas |  30.6 s |\n",
        "|            | dask   | 28.4 s |\n",
        "|            | spark  | 31.2 s |\n",
        "\n",
        "As you can see, pandas performances best. *So for handling small dataset, it is better to stay with pandas. Leave the dask and spark for large datasets.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YRYbf1GvFEu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "outputId": "5fd6d1f4-afba-43b6-ae3d-f1a09ca0c862"
      },
      "source": [
        "myobj = Agency_Loan(agency=\"FNMA\", \n",
        "                    acqYYYYQQ = \"2000Q1\", \n",
        "                    stageFolder = \"/content/drive/My Drive/ML_Data/stock\",\n",
        "                    acquisition_file = \"/content/drive/My Drive/ML_Data/stock/Acquisition_2000Q1.txt\",\n",
        "                    performance_file = \"/content/drive/My Drive/ML_Data/stock/Performance_2000Q1.txt\" )\n",
        "\n",
        "%time myobj.read_data_acquisition()\n",
        "%time myobj.read_data_performance()\n",
        "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n\")\n",
        "%time myobj.read_data_acquisition(save_as_parquet_file =\"pandas_LL_test.parquet\")\n",
        "\n",
        "%time myobj.read_data_performance(save_as_parquet_file =\"pandas_LP_test.parquet\")\n",
        "del myobj\n",
        "print(\"----------------------------------------------------------------------------------------------\\n\")\n",
        "myobj2 = Agency_Loan_Dask(agency=\"FNMA\", \n",
        "                    acqYYYYQQ = \"2000Q1\", \n",
        "                    stageFolder = \"/content/drive/My Drive/ML_Data/stock\",\n",
        "                    acquisition_file = \"/content/drive/My Drive/ML_Data/stock/Acquisition_2000Q1.txt\",\n",
        "                    performance_file = \"/content/drive/My Drive/ML_Data/stock/Performance_2000Q1.txt\" )\n",
        "\n",
        "%time myobj2.read_data_acquisition()\n",
        "%time myobj2.read_data_performance()\n",
        "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n\")\n",
        "%time myobj2.read_data_acquisition(save_as_parquet_file =\"dask_LL_test.parquet\")\n",
        "%time myobj2.read_data_performance(save_as_parquet_file =\"dask_PL_test.parquet\")\n",
        "del myobj2\n",
        "print(\"--------------------------------------------------------------------------------------------\\n\")\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField\n",
        "from pyspark.sql.types import DoubleType, LongType, StringType, DateType\n",
        "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "myobj3 = Agency_Loan_spark(agency=\"FNMA\",\n",
        "                            acqYYYYQQ = \"2000Q1\", \n",
        "                            stageFolder = \"/content/drive/My Drive/ML_Data/stock\",\n",
        "                            acquisition_file = \"/content/drive/My Drive/ML_Data/stock/Acquisition_2000Q1.txt\",\n",
        "                            performance_file = \"/content/drive/My Drive/ML_Data/stock/Performance_2000Q1.txt\" )\n",
        "%time myobj3.read_data_acquisition()\n",
        "%time myobj3.read_data_performance()\n",
        "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n\")\n",
        "%time myobj3.read_data_acquisition(save_as_parquet_file =\"spark_LL_test.parquet\")\n",
        "%time myobj3.read_data_performance(save_as_parquet_file =\"spark_PL_test.parquet\")\n",
        "del myobj3"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agency_Loan=>Loan_Data shape = ((246863, 25))\n",
            "'read_data_acquisition'  622.12 ms\n",
            "CPU times: user 598 ms, sys: 9.07 ms, total: 607 ms\n",
            "Wall time: 622 ms\n",
            "Agency_Loan=>performance_data shape = ((9128092, 14))\n",
            "'read_data_performance'  26899.98 ms\n",
            "CPU times: user 26 s, sys: 488 ms, total: 26.5 s\n",
            "Wall time: 26.9 s\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\n",
            "Agency_Loan=>Loan_Data shape = ((246863, 25))\n",
            "saved Loan data to pandas_LL_test.parquet\n",
            "'read_data_acquisition'  933.58 ms\n",
            "CPU times: user 899 ms, sys: 44.1 ms, total: 943 ms\n",
            "Wall time: 934 ms\n",
            "Agency_Loan=>performance_data shape = ((9128092, 14))\n",
            "saved Loan data to pandas_LP_test.parquet\n",
            "'read_data_performance'  30624.24 ms\n",
            "CPU times: user 29.7 s, sys: 858 ms, total: 30.5 s\n",
            "Wall time: 30.6 s\n",
            "----------------------------------------------------------------------------------------------\n",
            "\n",
            "'read_data_acquisition'  148.56 ms\n",
            "CPU times: user 143 ms, sys: 7.12 ms, total: 150 ms\n",
            "Wall time: 149 ms\n",
            "'read_data_performance'  197.66 ms\n",
            "CPU times: user 139 ms, sys: 52.1 ms, total: 191 ms\n",
            "Wall time: 198 ms\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\n",
            "saved Loan data to dask_LL_test.parquet\n",
            "'read_data_acquisition'  1546.61 ms\n",
            "CPU times: user 1.47 s, sys: 46.1 ms, total: 1.52 s\n",
            "Wall time: 1.55 s\n",
            "saved Loan data to dask_PL_test.parquet\n",
            "'read_data_performance'  28447.87 ms\n",
            "CPU times: user 47.5 s, sys: 2.03 s, total: 49.6 s\n",
            "Wall time: 28.4 s\n",
            "--------------------------------------------------------------------------------------------\n",
            "\n",
            "CPU times: user 12 ms, sys: 5.02 ms, total: 17 ms\n",
            "Wall time: 185 ms\n",
            "CPU times: user 20.7 ms, sys: 4.15 ms, total: 24.8 ms\n",
            "Wall time: 156 ms\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\n",
            "saved Loan data to spark_LL_test.parquet\n",
            "CPU times: user 21.5 ms, sys: 11.2 ms, total: 32.7 ms\n",
            "Wall time: 2.1 s\n",
            "saved Loan data to spark_PL_test.parquet\n",
            "CPU times: user 56.2 ms, sys: 8.91 ms, total: 65.1 ms\n",
            "Wall time: 31.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o60PHjUd9Mc",
        "colab_type": "code",
        "outputId": "1d1c9e2c-fc58-4669-a603-55c835325c4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "acqData.Performance_Data.filter(\"LOAN_AGE < 0\").select(\"LOAN_ID\", \"ACT_DTE\",\"LOAN_AGE\",  \"LAST_UPB\", \"SCHD_UPB\", \"DLQ_STATUS\").show(2000)\n",
        "round(15.555533355, 3)\n",
        "\n",
        " #if (scenario == 0):\n",
        "    #  myobj.read_data_acquisition(\"/content/drive/My Drive/ML_Data/stock/Acquisition_2000Q1.txt\")\n",
        "    #  mydata = myobj.Loan_Data\n",
        "    #  mydata.show(100)\n",
        "      #acq_df = spark.read.parquet(\"output/FNMA/LoanAcq.parquet\")\n",
        "      #acq_df.createOrReplaceTempView(\"acq_df_table\")\n",
        "      #a = spark.sql(\"select case when ORIG_CHN in ('R', 'C') then 'R1' when ORIG_CHN == 'B' then 'B1' else ORIG_CHN end as K, ORIG_CHN from acq_df_table where OCLTV is not null  \")\n",
        "      #a = spark.sql(\"select * from acq_df_table where ACQ = '\" + mydata.acqYYYYQQ + \"' limit 10\")\n",
        "      #a.show()\n",
        "      #a.printSchema()\n",
        "      #a = spark.sql(\"select * from acq_df_table order by  LOAN_ID, ACQ  limit 10\")\n",
        "      #a.show()\n",
        "     \n",
        "      #a = spark.sql(\"select count(*) from acq_df_table \") #where ACQ = '\" + mydata.acqYYYYQQ + \"' \")\n",
        "      #a.toPandas().head(100)\n",
        "     \n",
        "    #else:\n",
        "    #  mydata = myobj.read_data_performance(\"/content/drive/My Drive/ML_Data/stock/Performance_2000Q1.txt\")\n",
        "    #  mydata.show(100)\n",
        "      #per_df = spark.read.parquet(\"output/FNMA/LoanPerformance.parquet\")\n",
        "      #per_df.createOrReplaceTempView(\"per_df_table\")\n",
        "      #a = spark.sql(\"select case when ORIG_CHN in ('R', 'C') then 'R1' when ORIG_CHN == 'B' then 'B1' else ORIG_CHN end as K, ORIG_CHN from acq_df_table where OCLTV is not null  \")\n",
        "      #a = spark.sql(\"select * from per_df_table  limit 10\")\n",
        "      #a.show()\n",
        "      #a.printSchema()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------+--------+--------+--------+----------+\n",
            "|LOAN_ID|ACT_DTE|LOAN_AGE|LAST_UPB|SCHD_UPB|DLQ_STATUS|\n",
            "+-------+-------+--------+--------+--------+----------+\n",
            "+-------+-------+--------+--------+--------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubOOVlwD5D2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl_w80f-eWeE",
        "colab_type": "text"
      },
      "source": [
        "# Project Plans and Notes\n",
        "\n",
        "** package: LoanPerformance**\n",
        "\n",
        "1. Processing public loan performance (from FNMA and FRED)\n",
        "\n",
        "2. Doing Amortization. (UPB)\n",
        " \n",
        " 2.1 FRM\n",
        "\n",
        " 2.2 ARM (flexible rate)\n",
        " https://files.consumerfinance.gov/f/201204_CFPB_ARMs-brochure.pdf\n",
        "\n",
        " * initial rate and payment\n",
        " * The adjustment period\n",
        " * The index\n",
        " * The margin\n",
        " * Interest-Rate Caps\n",
        " * Payment Caps\n",
        "\n",
        " * Type of ARMS\n",
        "  * Hybrid\n",
        "  * Interest-only\n",
        "  * Payment-option\n",
        "  * stepped\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "3. Rate incentive if IR is provided\n",
        "\n",
        "4. MTMLTV is HP is provided\n",
        "\n",
        "5. Predicting Prepayment, Default, DQL12.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xzpEhgkkxUt",
        "colab_type": "text"
      },
      "source": [
        "# Private and Experimental codes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3DECpTrVokK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pfile = \"output/FNMA/LoanPerformance.parquet\"\n",
        "pfile = \"output/FNMA/LL_LoanPerformance.parquet\"\n",
        "ts = time.time()\n",
        "data = pd.read_parquet(pfile)\n",
        "#print(data)\n",
        "\n",
        "#print(data.info())\n",
        "\n",
        "#mdata = data[data.ZB_AGE  >0]\n",
        "\n",
        "#mm = data.DLQ_LAGs12.to_numpy()\n",
        "#print(mm)\n",
        "#print(type(mm))\n",
        "#print(\"transform: \" , showtime(ts))\n",
        "'''\n",
        "ts = time.time()\n",
        "per_df = spark.read.parquet(pfile)\n",
        "per_df.printSchema()\n",
        "#per_df.show(300)\n",
        "print(\"transform: \" , showtime(ts))\n",
        "print(data.info())\n",
        "my = per_df.select(\"ZB_DLQ_LAGs12\").flat()\n",
        "\n",
        "my.show(10)\n",
        "'''\n",
        "\n",
        "\n",
        "def convert_array_elements_as_dataframe(df, colname, requested_size):\n",
        " \n",
        "    #V = df[colname].apply(lambda x: [] if x== np.NaN else x)\n",
        "    df[colname].fillna(\"\", inplace=True)\n",
        "    df[colname + \"_OrigSize\"] = df[colname].apply(lambda x: len(x))\n",
        "    V = df[colname].apply(lambda x: (([-1]*(12-len(x))) + list(x)) if   len(x) < 12 else x  )\n",
        "    V =V.to_numpy().flatten()\n",
        "    V = np.vstack(V)\n",
        "    result_df = pd.DataFrame(V)\n",
        "    result_df[\"LOAN_ID\"] = df[\"LOAN_ID\"]\n",
        "\n",
        "    df = pd.merge(df, result_df, how=\"inner\", on=\"LOAN_ID\")\n",
        "    return (df)\n",
        "    \n",
        "\n",
        "v = convert_array_elements_as_dataframe(data, \"F3Q_DLQ_LAGs12\", 12)\n",
        "\n",
        "R = v.query(\"F3Q_AGE <12 \")\n",
        "#data[ ((data[\"F3Q_AGE\"] > 0) & (data[\"F3Q_AGE\"] < 12)) ][[\"LOAN_ID\", \"F3Q_DLQ_LAGs12\"]]\n",
        "#mydata = data.loc[ ((data[\"F3Q_AGE\"] > 0) & (data[\"F3Q_AGE\"] < 12)) ,[\"LOAN_ID\", \"F3Q_DLQ_LAGs12\"]]\n",
        "#mydata[\"NumSize\"] = mydata[\"F3Q_DLQ_LAGs12\"].apply(lambda x: len(x))\n",
        "\n",
        "#mydata[\"F3Q_DLQ_LAGs12\"] = mydata[\"F3Q_DLQ_LAGs12\"].apply(lambda x: (([-1]*(12-len(x))) + list(x)) if len(x) < 12 else x  )\n",
        "\n",
        "\n",
        "#mydata[[\"F3Q_DLQ_LAGs12\"]]\n",
        "\n",
        "\n",
        "\n",
        "#dd = mydata[[\"F3Q_DLQ_LAGs12\"]].to_numpy().flatten()\n",
        "#dd = np.vstack(dd)\n",
        "#print(dd.shape)\n",
        "#m =[]\n",
        "#for d in dd:\n",
        "#   m.append(list(d))\n",
        "\n",
        "#print(m.flatten())\n",
        "#mm = np.concatenate(m)\n",
        "#print(\"DDDDDDD\")\n",
        "#print(mm)\n",
        "#print(mm.shape)\n",
        "\n",
        "#a = mydata.to_numpy().apply(lambda x: pd.DataFrame({\"A\": x[0], \"B\": x[1]}), axis=1 ) #[\"LOAN_ID\"], x[\"F3Q_DLQ_LAGs12\"]))\n",
        "\n",
        "#print(type(a))\n",
        "\n",
        "#pa=pd.DataFrame({'a':[[1.,4.],  [2.],             [3.,4.,5.]]})\n",
        "#Q = pa.to_numpy().flatten()\n",
        "#print(Q)\n",
        "\n",
        "R"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suygpFAN0GeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install numpy_financial\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy_financial as npf\n",
        "import time\n",
        "\n",
        "pfile = \"output/FNMA/LoanAcq.parquet\"\n",
        "ts = time.time()\n",
        "\n",
        "#gapminder.assign(pop_in_millions=lambda x: x['pop']/1e6,\n",
        "#                pop_in_billions=lambda x: x['pop_in_millions']/1e3).head()\n",
        "\n",
        "ts = time.time()\n",
        "data = pd.read_parquet(pfile)\n",
        "print(\"read_parquet: \" , showtime(ts))\n",
        "print(data[\"ORIG_RT\"][1:10])\n",
        "\n",
        "#@timeit2\n",
        "def test1(data):\n",
        "    def compute_schd_upb(self, rates, num_months, principals, max_month = None):\n",
        "        print(f\"{principals[0]} : {num_months[0]} : {rates[0]*1200}\")\n",
        "        print(f\"{principals[1]} : {num_months[1]} : {rates[1]*1200}\")\n",
        "        #expect acq_data is a pandas dataframe, having fields\n",
        "        #   LOAN_ID, ORIG_RT, ORIG_AMT, ORIG_TRM\n",
        "        num_loans = rates.count()\n",
        "      \n",
        "        #using numpy array for now\n",
        "        num_month = num_months.max()\n",
        "\n",
        "        if max_month is not None:\n",
        "           num_month = max(1, max_month)\n",
        "\n",
        "        upb_matrix = np.zeros((num_loans, num_month))\n",
        "        upb_matrix[:, 0] = principals\n",
        "        for i in range(1, num_month):\n",
        "            p_payment = -npf.ppmt(rate =rates , per=i, nper=num_months, pv= principals)\n",
        "            upb_matrix[:, i] =  upb_matrix[:, i-1] -  p_payment\n",
        "\n",
        "        return upb_matrix\n",
        "     \n",
        "    \n",
        "    r = compute_schd_upb(0, rates      = data[\"ORIG_RT\"] / 1200,\n",
        "                            num_months = data[\"ORIG_TRM\"],\n",
        "                            principals = data[\"ORIG_AMT\"] )\n",
        "    #print(r[1, 0:185])\n",
        "    return r.shape\n",
        "\n",
        "#@timeit2\n",
        "def test2(data):\n",
        "   \n",
        "    def compute_schd_upb(self, rates, num_months, principals, max_month = None):\n",
        "        \n",
        "        #print(f\"{principals[0]} : {num_months[0]} : {rates[0]*1200}\")\n",
        "        #print(f\"{principals[1]} : {num_months[1]} : {rates[1]*1200}\")\n",
        "        #expect acq_data is a pandas dataframe, having fields\n",
        "        #   LOAN_ID, ORIG_RT, ORIG_AMT, ORIG_TRM\n",
        "        num_loans = rates.count()\n",
        "        #using numpy array for now\n",
        "        num_month = num_months.max()\n",
        "\n",
        "        if max_month is not None:\n",
        "           num_month = max(1, max_month)\n",
        "\n",
        "        upb_matrix = np.zeros((num_loans, num_month))\n",
        "        upb_matrix[:, 0] = principals\n",
        "        total_payments = principals * (rates / (1 - (1 + rates) ** (-num_months)))\n",
        "        for i in range(1, num_month):\n",
        "            #total_payments = principals * (rates / (1 - (1 + rates) ** (-num_months)))\n",
        "            p_payment = total_payments - upb_matrix[:, i-1]*rates\n",
        "            #p_payment = -npf.ppmt(rate =rates , per=i, nper=num_months, pv= principals)\n",
        "            upb_matrix[:, i] =  upb_matrix[:, i-1] -  p_payment\n",
        "            #upb_matrix[:, i] =  upb_matrix[:, i-1] * (1+ rates) - total_payments\n",
        "        return upb_matrix\n",
        "     \n",
        "    \n",
        "    r = compute_schd_upb(0, rates      = data[\"ORIG_RT\"] / 1200,\n",
        "                            num_months = data[\"ORIG_TRM\"],\n",
        "                            principals = data[\"ORIG_AMT\"] )\n",
        "    #print(r[1, 1:20])\n",
        "    return r.shape\n",
        "\n",
        "def compute_amortization(principals, monthly_rates, terms,  start_period = 0, end_period = None):\n",
        "    \"\"\"\n",
        "    Compute amortization of loans\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    principals : scalar or array_like of shape(M, )\n",
        "        pricipals of loans\n",
        "    monthly_rates: scalar or array_like if  principals is a scalar\n",
        "                   or\n",
        "                   array_like or matrix_like if principals is an array_like\n",
        "        For FRM, one Rate for each loan\n",
        "        For ARM, one full time series for each loan\n",
        "    \n",
        "    terms:  scalar or array_like of shape(M, )\n",
        "    loan terms, type should match that of principals\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    out : ndarray (M, N)\n",
        "\n",
        "    \"\"\"\n",
        "    num_loans = 1\n",
        "    if isinstance(principals, pd.Series):\n",
        "       principals = principals.values\n",
        "    elif np.ndim(principals) == 0:\n",
        "       principals = [principals]\n",
        "    #assume principals is np.array\n",
        "    num_loans = len(principals)\n",
        "\n",
        "    if isinstance(monthly_rates, pd.Series):\n",
        "       monthly_rates = monthly_rates.values\n",
        "    elif np.ndim(principals) == 0:\n",
        "       monthly_rates = [monthly_rates]\n",
        "\n",
        "    if isinstance(terms, pd.Series):\n",
        "       terms = terms.values\n",
        "    elif np.ndim(terms) == 0:\n",
        "       terms = [terms]\n",
        "    \n",
        "    num_month = terms.max()\n",
        "    mini_term = terms.min()\n",
        "    if end_period is not None:\n",
        "      num_month = min(num_month, max(1, (end_period - start_period)))\n",
        "    \n",
        "    upb_matrix = np.zeros((num_month, num_loans))\n",
        "    the_payment = principals * (monthly_rates / (1 - (1 + monthly_rates) ** (-terms)))\n",
        "    \n",
        "    current_upb = principals\n",
        "    if start_period > 0:\n",
        "       current_upb = principals\n",
        "       for t in range(1, start_period+1):\n",
        "          pp_payment = the_payment - current_upb * monthly_rates\n",
        "          current_upb = current_upb - pp_payment\n",
        "    \n",
        "    upb_matrix[0, :] = current_upb\n",
        "      \n",
        "    i = 1\n",
        "    for t in range(start_period+1, start_period +num_month ):\n",
        "       isbeyondTerm = np.greater(t , terms)\n",
        "       pp_payment = the_payment - upb_matrix[i-1, :] * monthly_rates\n",
        "       upb_matrix[i, :] = (upb_matrix[i-1, :] - pp_payment) \n",
        "       if t >= mini_term:\n",
        "          iswithinTerm = np.greater( terms, t).astype(int)\n",
        "          isbeyondTerm = np.greater(t , terms).astype(int)\n",
        "          upb_matrix[i, :] = upb_matrix[i, :] * np.array(iswithinTerm) + (-999) * isbeyondTerm\n",
        "       i = i +  1\n",
        "    \n",
        "    ##post processing\n",
        "    #1, The UPB beyond its term is set to be -999\n",
        "    #i = 1\n",
        "    #for t in range(start_period+1, start_period +num_month ):\n",
        "    #   isbeyondTerm = np.greater(t , terms)\n",
        "    #   upb_matrix[i, :] = upb_matrix[i, :] + (-999) * np.array(isbeyondTerm)\n",
        "    #   i = i +  1\n",
        "\n",
        "    return upb_matrix.T\n",
        "\n",
        "\n",
        "%timeit cal_amortization(principals    = data[\"ORIG_AMT\"], \\\n",
        "                     monthly_rates = data[\"ORIG_RT\"] / 1200, \\\n",
        "                     terms         = data[\"ORIG_TRM\"])\n",
        "\n",
        "for i, x in enumerate(r[1, :]):\n",
        "  print(i, \"  \", x)\n",
        "#%timeit test2(data)\n",
        "'''\n",
        " 174         3,556.73     521.65     498.31      23.34       3,058.42\n",
        " 175         3,058.42     521.65     501.58      20.07       2,556.84\n",
        " 176         2,556.84     521.65     504.87      16.78       2,051.97\n",
        " 177         2,051.97     521.65     508.18      13.47       1,543.79\n",
        " 178         1,543.79     521.65     511.52      10.13       1,032.27\n",
        " 179         1,032.27     521.65     514.88       6.77         517.39\n",
        " 180           517.39     521.65     518.25       3.40          -0.86\n",
        "       \n",
        "  1         55,000.00     521.65     160.71     360.94      54,839.29\n",
        "  2         54,839.29     521.65     161.77     359.88      54,677.52\n",
        "  3         54,677.52     521.65     162.83     358.82      54,514.69\n",
        "  4         54,514.69     521.65     163.90     357.75      54,350.79\n",
        "  5      \n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Akb3tC7LhGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, absolute_import, print_function\n",
        "r = 0.06/12\n",
        "n =240\n",
        "s = 150000\n",
        "p = r*s*(1+r)**(n) /(1-(1+r)**(n))\n",
        "p\n",
        "\n",
        "\n",
        "from decimal import *\n",
        "\n",
        "def amortization_table(principal, rate, term):\n",
        "    ''' Prints the amortization table for a loan.\n",
        "\n",
        "    Prints the amortization table for a loan given\n",
        "    the principal, the interest rate (as an APR), and\n",
        "    the term (in months).'''\n",
        "\n",
        "    payment = pmt(principal, rate, term)\n",
        "    begBal = principal\n",
        "\n",
        "    # Print headers\n",
        "    print ('Pmt no'.rjust(6), ' ', 'Beg. bal.'.ljust(13), ' ',  end = '')\n",
        "    print ('Payment'.ljust(9), ' ', 'Principal'.ljust(9), ' ',  end = '')\n",
        "    print ('Interest'.ljust(9), ' ', 'End. bal.'.ljust(13),  end = '')\n",
        "    print (''.rjust(6, '-'), ' ', ''.ljust(13, '-'), ' ',  end = '')\n",
        "    print (''.rjust(9, '-'), ' ', ''.ljust(9, '-'), ' ',  end = '')\n",
        "    print (''.rjust(9, '-'), ' ', ''.ljust(13, '-'), ' ')\n",
        "    # Print data\n",
        "    for num in range(1, term + 1):\n",
        "        \n",
        "        interest = round(begBal * (rate / (12 * 100.0)), 2)\n",
        "        applied = (payment - interest)\n",
        "        endBal = (begBal - applied)\n",
        "        \n",
        "        print (str(num).center(6), ' ',  end = '')\n",
        "        print ('{0:,.2f}'.format(begBal).rjust(13), ' ',  end = ''),\n",
        "        print ('{0:,.2f}'.format(payment).rjust(9), ' ', end = ''),\n",
        "        print ('{0:,.2f}'.format(applied).rjust(9), ' ',  end = ''),\n",
        "        print ('{0:,.2f}'.format(interest).rjust(9), ' ', end = ''),\n",
        "        print ('{0:,.2f}'.format(endBal).rjust(13))\n",
        "\n",
        "        begBal = endBal\n",
        "    \n",
        "def pmt(principal, rate, term):\n",
        "    '''Calculates the payment on a loan.\n",
        "\n",
        "    Returns the payment amount on a loan given\n",
        "    the principal, the interest rate (as an APR),\n",
        "    and the term (in months).'''\n",
        "    \n",
        "    ratePerTwelve = rate / (12 * 100.0)\n",
        "    \n",
        "    result = principal * (ratePerTwelve / (1 - (1 + ratePerTwelve) ** (-term)))\n",
        "\n",
        "    # Convert to decimal and round off to two decimal\n",
        "    # places.\n",
        "    result = (result)\n",
        "    #result = round(result, 2)\n",
        "    return result\n",
        "\n",
        "amortization_table(55000, 7.875, 180)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}